{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inproject.constants import *\n",
    "from src.inproject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_pathway=CONFIG_FILE_PATH, \n",
    "                 param_pathway=PARAM_FILE_PATH, \n",
    "                 schema_pathway=SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_pathway)\n",
    "        self.param = read_yaml(param_pathway)\n",
    "        self.schema = read_yaml(schema_pathway)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self)->DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path\n",
    "        )\n",
    "        return data_transformation_config\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inproject.logging import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# import os\n",
    "# import logging\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def remove_outlier(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        data = data.drop(columns='Policy Start Date')\n",
    "        train, test = train_test_split(data, test_size=0.20, random_state=42)\n",
    "\n",
    "        cat_col = data.select_dtypes(include=['object']).columns.tolist()\n",
    "        num_col = [col for col in train.select_dtypes(exclude=['object']).columns if col != 'target']  # Exclude target\n",
    "        num_col = num_col[1:]\n",
    "        print(f\"Numerical Columns: {num_col}\")\n",
    "        print(f\"Categorical Columns: {cat_col}\")\n",
    "\n",
    "        for col in num_col:\n",
    "            Q1 = train[col].quantile(0.25)\n",
    "            Q3 = train[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            train = train[(train[col] >= lower_bound) & (train[col] <= upper_bound)]\n",
    "\n",
    "        print(f\"Train shape after outlier removal: {train.shape}\")\n",
    "        print(f\"Test shape after outlier removal: {test.shape}\")\n",
    "\n",
    "        return train, test, cat_col, num_col\n",
    "    \n",
    "    def replace_missing_values(self, train, test, cat_col, num_col):\n",
    "        num_imputer = SimpleImputer(strategy='mean')\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "        train[num_col] = num_imputer.fit_transform(train[num_col])\n",
    "        test[num_col] = num_imputer.transform(test[num_col])\n",
    "\n",
    "        train[cat_col] = cat_imputer.fit_transform(train[cat_col])\n",
    "        test[cat_col] = cat_imputer.transform(test[cat_col])\n",
    "\n",
    "        print(f\"Train shape after missing value imputation: {train.shape}\")\n",
    "        print(f\"Test shape after missing value imputation: {test.shape}\")\n",
    "\n",
    "        return train, test\n",
    "\n",
    "    def label_encoding_categorical(self, train, test, cat_col):\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # Iterate over each categorical column and apply LabelEncoder\n",
    "        for col in cat_col:\n",
    "            train[col] = label_encoder.fit_transform(train[col])\n",
    "            test[col] = label_encoder.transform(test[col])\n",
    "\n",
    "        return train, test\n",
    "    \n",
    "    def scaling(self, train, test, num_col):\n",
    "        scaler = StandardScaler()\n",
    "        train[num_col] = scaler.fit_transform(train[num_col])\n",
    "        test[num_col] = scaler.transform(test[num_col])\n",
    "\n",
    "        # Save the scaled train and test sets\n",
    "        train.to_csv(os.path.join(self.config.root_dir, 'train.csv'), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, 'test.csv'), index=False)\n",
    "\n",
    "        # Logging\n",
    "        logger.info(\"Split into training and test sets, with scaling applied.\")\n",
    "        logger.info(f\"Train shape: {train.shape}\")\n",
    "        logger.info(f\"Test shape: {test.shape}\")\n",
    "\n",
    "        print(f\"Train shape after scaling: {train.shape}\")\n",
    "        print(f\"Test shape after scaling: {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    train, test, cat_col, num_col = data_transformation.remove_outlier()\n",
    "    train, test = data_transformation.replace_missing_values(train, test, cat_col, num_col)\n",
    "    train, test = data_transformation.label_encoding_categorical(train, test, cat_col)\n",
    "    data_transformation.scaling(train, test, num_col)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
